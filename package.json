{
  "name": "seoserver",
  "version": "1.1.6",
  "main": "lib/seoserver",
  "bin": {
    "seoserver": "./bin/seoserver.js"
  },
  "dependencies": {
    "commander": "~1.0.2",
    "forever-monitor": "~1.1.0",
    "express": "~3.0.0rc4",
    "memcached": "0.1.5"
  },
  "readme": "  <h3>Welcome!</h3>\n  <p>Seo Server is a command line tool that runs a server that allows GoogleBot(and any other crawlers) to crawl your heavily Javascript built websites. The tool works with very little changes to your server or client side code.</p>\n  <p><i>This entire site is driven by Javascript(view the source or see the <a href=\"https://github.com/apiengine/seoserver-site\">code</a>). Click the `What does Google see?` button at the bottom of each page to see Seo Server in action.</i></p>\n\n  <h3>How it works</h3>\n  <img src=\"http://yuml.me/5b1b60bb\" /><br /><br />\n  <p>Seo Server runs <a href=\"http://phantomjs.org/\">PhantomJs</a>(headless webkit browser) which renders the page fully and returns the fully executed code to GoogleBot.</p>\n  \n  <h3>Getting started</h3>\n  <p>1) you must install PhantomJs(<a href=\"http://phantomjs.org/\">http://phantomjs.org/</a>) and link into your bin so that Seo Server can call it.</p>\n  <p>2) Seo Server is an NPM module so install via</p>\n  <code>sudo npm install -g seoserver</code>\n  <p>3) Now we have access to the Seo Server command line tool</p>\n  <code>seoserver start</code>\n  <p>Which starts an Express server on port 3000 or</p>\n  <code> seoserver -p 4000 start</code> \n  <p>Start it as a background process and log the output</p>\n  <code> seoserver -p 4000 start > seoserver.log &</code> \n\n  <h3>Telling GoogleBot to fetch from Seo Server</h3>\n  <p>To tell GoogleBot that we are using ajaxed content we simply add to our sites index.html file the Google specific <a href=\"https://developers.google.com/webmasters/ajax-crawling/docs/specification\">meta tag</a>. If you view the source of this page you can see we have included the tag below. </p>\n  <code>&lt;meta name=\"fragment\" content=\"!\"&gt;</code>\n  <p>Now whenever GoogleBot visits any of our pages it will try to load <code>?_escaped_fragment_=pathname</code></p>\n  <p>So if we were using Apache with mod rewrite and mod proxy, we can include in our .htaccess</p>\n  <code>\n    RewriteCond %{QUERY_STRING} ^_escaped_fragment_=(.*)$<br />\n    RewriteRule (.*) http://address-of-seoserver:3000/%1? [P]\n  </code>\n  <p>Now all request from GoogleBot will be returned fully rendered. How GoogleBot sees the page can be tested with Google <a href=\"http://www.google.com/webmasters/\">WebMasters</a>(they allow you to simulate Google crawls and see the result instantly).</p>\n\n  <h3>For other crawlers</h3>\n  <p>\n    Using mod rewrite, we can send other crawlers to Seo Server also\n  </p>\n  <code>\n    RewriteCond %{HTTP_USER_AGENT} ^DuckDuckBot/1.0;<br />\n    RewriteRule (.*) http://address-of-seoserver:3000/%1? [P]\n\n  </code>\n  <h3>FAQ</h3>\n  <p>Nothing here yet, but check out the examples on the left to see different types of ajaxed content. Also ask questions and give feedback on GitHub <a href=\"https://github.com/apiengine/seoserver/issues\">issues</a>.",
  "_id": "seoserver@1.1.6",
  "description": "<h3>Welcome!</h3>   <p>Seo Server is a command line tool that runs a server that allows GoogleBot(and any other crawlers) to crawl your heavily Javascript built websites. The tool works with very little changes to your server or client side code.</p>   <p><i>This entire site is driven by Javascript(view the source or see the <a href=\"https://github.com/apiengine/seoserver-site\">code</a>). Click the `What does Google see?` button at the bottom of each page to see Seo Server in action.</i></p>",
  "dist": {
    "shasum": "f13e2cd380a33e310e510cd2c6bae20489dabc3e"
  },
  "_from": "seoserver"
}
